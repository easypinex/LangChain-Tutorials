{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref: https://medium.com/neo4j/implementing-from-local-to-global-graphrag-with-neo4j-and-langchain-constructing-the-graph-73924cc5bab4\n",
    "### Ref(Github): https://github.com/tomasonjo/blogs/blob/master/llm/ms_graphrag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"2wsx3edc\"\n",
    "\n",
    "database = os.environ.get('NEO4J_DATABASE')\n",
    "graph = Neo4jGraph(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint='https://sales-chatbot-llm.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15',\n",
    "    azure_deployment='text-embedding-3-small',\n",
    "    openai_api_version='2023-05-15'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import Neo4jVector\n",
    "# # ! pip3 install -U langchain-huggingface\n",
    "# import os\n",
    "# os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/storage/models/embedding_models'\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# # Choose from https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# # embedding = HuggingFaceEmbeddings(model_name=\"lier007/xiaobu-embedding-v2\")\n",
    "\n",
    "# model_path = os.path.join(os.environ['SENTENCE_TRANSFORMERS_HOME'], 'models--lier007--xiaobu-embedding-v2/snapshots/ee0b4ecdf5eb449e8240f2e3de2e10eeae877691')\n",
    "# embedding = HuggingFaceEmbeddings(model_name=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_label = '__Entity__'\n",
    "# embedding_node_property = 'embedding'\n",
    "# fetch_query = (\n",
    "#     f\"MATCH (n:`{node_label}`) \"\n",
    "#     f\"WHERE n.{embedding_node_property} IS null \"\n",
    "#     \"AND any(k in $props WHERE n[k] IS NOT null) \"\n",
    "#     f\"RETURN elementId(n) AS id, reduce(str='',\"\n",
    "#     \"k IN $props | str + '\\\\n' + k + ':' + coalesce(n[k], '')) AS text \"\n",
    "#     \"LIMIT 1000\"\n",
    "# )\n",
    "# datas = graph.query(fetch_query, params={\"props\": ['id', 'description']})\n",
    "# datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from tools.TokenCounter import num_tokens_from_string\n",
    "\n",
    "# tokens_num = 0\n",
    "# for data in datas:\n",
    "#     tokens_num += num_tokens_from_string(data['text'])\n",
    "# tokens_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Neo4jVector.from_existing_graph(\n",
    "    embedding,\n",
    "    index_name='embedding',\n",
    "    node_label='__Entity__',\n",
    "    text_node_properties=['id', 'description'],\n",
    "    embedding_node_property='embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience \n",
    "\n",
    "gds = GraphDataScience( \n",
    "    os.environ[ \"NEO4J_URI\" ], \n",
    "    auth=(os.environ[ \"NEO4J_USERNAME\" ], os.environ[ \"NEO4J_PASSWORD\" ]) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graphName                                                         entities\n",
       "database                                                             neo4j\n",
       "databaseLocation                                                     local\n",
       "memoryUsage                                                               \n",
       "sizeInBytes                                                             -1\n",
       "nodeCount                                                               26\n",
       "relationshipCount                                                       59\n",
       "configuration            {'relationshipProjection': {'__ALL__': {'aggre...\n",
       "density                                                           0.090769\n",
       "creationTime                           2024-09-18T04:48:59.916202970+00:00\n",
       "modificationTime                       2024-09-18T04:49:02.053575346+00:00\n",
       "schema                   {'graphProperties': {}, 'nodes': {'__Entity__'...\n",
       "schemaWithOrientation    {'graphProperties': {}, 'nodes': {'__Entity__'...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.graph.drop(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"entities\",                   # Graph name\n",
    "    \"__Entity__\",                 # Node projection\n",
    "    \"*\",                          # Relationship projection\n",
    "    nodeProperties=[\"embedding\"]  # Configuration parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranIterations                                                             7\n",
       "nodePairsConsidered                                                  158858\n",
       "didConverge                                                            True\n",
       "preProcessingMillis                                                       5\n",
       "computeMillis                                                           287\n",
       "mutateMillis                                                             51\n",
       "postProcessingMillis                                                      0\n",
       "nodesCompared                                                           407\n",
       "relationshipsWritten                                                    142\n",
       "similarityDistribution    {'min': 0.9501686096191406, 'p5': 0.9522705078...\n",
       "configuration             {'mutateProperty': 'score', 'jobId': '68c7d7c9...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 gds.knn.mutate 根據嵌入向量相似度創建關聯關係\n",
    "\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "gds.knn.mutate(\n",
    "  G,\n",
    "  nodeProperties=['embedding'],\n",
    "  mutateRelationshipType= 'SIMILAR',\n",
    "  mutateProperty= 'score',\n",
    "  similarityCutoff=similarity_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeMillis                                                             33\n",
       "nodePropertiesWritten                                                  407\n",
       "componentCount                                                         368\n",
       "componentDistribution    {'min': 1, 'p5': 1, 'max': 10, 'p999': 10, 'p9...\n",
       "postProcessingMillis                                                    27\n",
       "preProcessingMillis                                                      0\n",
       "computeMillis                                                            6\n",
       "configuration            {'writeProperty': 'wcc', 'jobId': 'e65b0452-dc...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 gds.wcc.write 將相似節點進行社群劃分\n",
    "# writeProperty=\"wcc\": 為每個節點寫入 wcc 屬性，表示該節點屬於哪個社群。\n",
    "\n",
    "gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty=\"wcc\",\n",
    "    relationshipTypes=[\"SIMILAR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'combinedResult': ['台幣續期保險費', '台幣首期保險費']},\n",
       " {'combinedResult': ['郵局劃撥', '郵局劃撥_公司']},\n",
       " {'combinedResult': ['行動保全續期保費服務', '行動保全續期保費服務說明']},\n",
       " {'combinedResult': ['原信用卡授權書之申請日', '新信用卡授權書之申請日']},\n",
       " {'combinedResult': ['首期保險費', '首期保險費金額']},\n",
       " {'combinedResult': ['傷害險主約', '傷害險主約_信用卡']},\n",
       " {'combinedResult': ['核印失敗', '請款失敗']},\n",
       " {'combinedResult': ['E-Bill 全國繳費網', 'E-Bill全國繳費網']},\n",
       " {'combinedResult': ['保單生效日為109年7月1日(不含)以前', '保單生效日為109年7月1日(含)以後']},\n",
       " {'combinedResult': ['保單生效日_109年7月1日以前', '保單生效日_109年7月1日以後']},\n",
       " {'combinedResult': ['0歲~30歲',\n",
       "   '0歲~40歲',\n",
       "   '31歲~40歲',\n",
       "   '41歲~50歲',\n",
       "   '41歲~70歲',\n",
       "   '51歲~60歲',\n",
       "   '61歲~70歲',\n",
       "   '71歲~90歲']},\n",
       " {'combinedResult': ['71歲以上', '91歲以上']},\n",
       " {'combinedResult': ['31-40歲', '41-50歲', '51-60歲', '61-70歲', '71-90歲']},\n",
       " {'combinedResult': ['死亡給付_101%', '死亡給付_115%', '死亡給付_130%']},\n",
       " {'combinedResult': ['當月10日', '當月11日']},\n",
       " {'combinedResult': ['當月25日', '當月25日午夜']}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查找具有潛在重複 ID 的節點\n",
    "word_edit_distance = 3\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`__Entity__`)\n",
    "    WHERE size(e.id) > 3 // longer than 3 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance \n",
    "                  OR node.id CONTAINS n.id | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\", params={'distance': word_edit_distance})\n",
    "potential_duplicate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('potential_duplicate_candidates.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(potential_duplicate_candidates, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# model_name = 'qwen2:72b-instruct-q8_0'\n",
    "# llm = OllamaFunctions(model=model_name, temperature=0)\n",
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a data processing assistant. Your task is to identify duplicate entities in a list and decide which of them should be merged.\n",
    "The entities might be slightly different in format or content, but essentially refer to the same thing. Use your analytical skills to determine duplicates.\n",
    "\n",
    "Here are the rules for identifying duplicates:\n",
    "1. Entities with minor typographical differences should be considered duplicates, except when they refer to differences such as \"new\" vs. \"old,\" or \"initial\" vs. \"renewal.\" In these cases, do not merge the results.\n",
    "2. Entities with different formats but the same content should be considered duplicates.\n",
    "3. Entities that refer to the same real-world object or concept, even if described differently, should be considered duplicates.\n",
    "4. If it refers to different numbers, dates, or products, do not merge results\n",
    "\"\"\"\n",
    "user_template = \"\"\"\n",
    "Here is the list of entities to process:\n",
    "{entities}\n",
    "\n",
    "Please identify duplicates, merge them, and provide the merged list.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DuplicateEntities(BaseModel):\n",
    "    entities: List[str] = Field(\n",
    "        description=\"Entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Disambiguate(BaseModel):\n",
    "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
    "        description=\"Lists of entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "extraction_llm = llm.with_structured_output(\n",
    "    Disambiguate\n",
    ")\n",
    "\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = extraction_prompt | extraction_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  25%|██▌       | 4/16 [00:00<00:02,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process element faild!:['原信用卡授權書之申請日', '新信用卡授權書之申請日'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['台幣續期保險費', '台幣首期保險費'], error:\n",
      "'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  56%|█████▋    | 9/16 [00:01<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process element faild!:['傷害險主約', '傷害險主約_信用卡'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['核印失敗', '請款失敗'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['保單生效日為109年7月1日(不含)以前', '保單生效日為109年7月1日(含)以後'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['保單生效日_109年7月1日以前', '保單生效日_109年7月1日以後'], error:\n",
      "'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  81%|████████▏ | 13/16 [00:01<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process element faild!:['71歲以上', '91歲以上'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['31-40歲', '41-50歲', '51-60歲', '61-70歲', '71-90歲'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['死亡給付_101%', '死亡給付_115%', '死亡給付_130%'], error:\n",
      "'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  94%|█████████▍| 15/16 [00:01<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process element faild!:['當月10日', '當月11日'], error:\n",
      "'NoneType' object is not iterable\n",
      "process element faild!:['0歲~30歲', '0歲~40歲', '31歲~40歲', '41歲~50歲', '41歲~70歲', '51歲~60歲', '61歲~70歲', '71歲~90歲'], error:\n",
      "'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 16/16 [00:02<00:00,  7.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resolve_and_merge_entities_with_llm(potential_duplicate_candidates, max_retry=0) -> List[List[str]]:\n",
    "    '''\n",
    "    parmas:\n",
    "        potential_duplicate_candidates(List[dict['combinedResult': List[str]]): 有可能需要合併的清單 \n",
    "                                                                                e.g.[{'combinedResult': ['土地銀行', '第一銀行']}]\n",
    "        max_retry: 最多嘗試次數, 假設為2, 則最多遞迴執行 2+1=3次\n",
    "    return:\n",
    "        merged_entities (List[dict['combinedResult': List[str]]) : LLM 確認過需要合併的清單\n",
    "                                                                    e.g.[{'combinedResult': ['土地銀行', '第一銀行']}]\n",
    "    '''\n",
    "    def entity_resolution(entities: List[str]) -> Optional[List[List[str]]]:\n",
    "        return [\n",
    "            el.entities\n",
    "            for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
    "        ]\n",
    "        \n",
    "    merged_entities_result = []\n",
    "    merged_future_map = {}\n",
    "    futures = []\n",
    "    merged_failds = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submitting all tasks and creating a list of future objects\n",
    "        for el in potential_duplicate_candidates:\n",
    "            future = executor.submit(entity_resolution, el['combinedResult'])\n",
    "            merged_future_map[future] = el\n",
    "            futures.append(future)\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "        ):\n",
    "            try:\n",
    "                to_merge = future.result()\n",
    "                if to_merge:\n",
    "                    merged_entities_result.extend(to_merge)\n",
    "            except Exception as e:\n",
    "                el = merged_future_map[future]\n",
    "                print(f'process element faild!:{el['combinedResult']}, error:\\n{e}')\n",
    "                merged_failds.append(el)\n",
    "    if len(merged_failds) > 0 and max_retry > 0:\n",
    "        merged_entities_result.extend(resolve_and_merge_entities_with_llm(merged_failds, max_retry=max_retry-1))\n",
    "    return merged_entities_result\n",
    "merged_entities = resolve_and_merge_entities_with_llm(potential_duplicate_candidates, max_retry=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['郵局劃撥', '郵局劃撥_公司'],\n",
       " ['首期保險費', '首期保險費金額'],\n",
       " ['行動保全續期保費服務', '行動保全續期保費服務說明'],\n",
       " ['E-Bill 全國繳費網', 'E-Bill全國繳費網'],\n",
       " ['當月25日', '當月25日午夜']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for merge_entity in merged_entities:\n",
    "    merge_entities = [merge_entity]\n",
    "    results = graph.query(\"\"\"\n",
    "  UNWIND $data AS candidates\n",
    "  CALL {\n",
    "    WITH candidates\n",
    "    MATCH (e:__Entity__) WHERE e.id IN candidates\n",
    "    RETURN collect(e) AS nodes\n",
    "  }\n",
    "  CALL apoc.refactor.mergeNodes(nodes, {\n",
    "      properties: {\n",
    "        description: 'combine',\n",
    "        `.*`: 'discard'\n",
    "      }\n",
    "    })\n",
    "  YIELD node\n",
    "  RETURN count(*)\n",
    "  \"\"\", params={\"data\": merge_entities})\n",
    "    count += results[0]['count(*)']\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
