{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"2wsx3edc\"\n",
    "database = os.environ.get('NEO4J_DATABASE')\n",
    "graph = Neo4jGraph(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    azure_endpoint='https://sales-chatbot-llm.openai.azure.com/openai/deployments/embedding-ada-002/embeddings?api-version=2023-05-15',\n",
    "    azure_deployment='text-embedding-ada-002',\n",
    "    openai_api_version='2023-05-15'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import Neo4jVector\n",
    "# # ! pip3 install -U langchain-huggingface\n",
    "# import os\n",
    "# os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/storage/models/embedding_models'\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# # Choose from https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# # embedding = HuggingFaceEmbeddings(model_name=\"lier007/xiaobu-embedding-v2\")\n",
    "\n",
    "# model_path = os.path.join(os.environ['SENTENCE_TRANSFORMERS_HOME'], 'models--lier007--xiaobu-embedding-v2/snapshots/ee0b4ecdf5eb449e8240f2e3de2e10eeae877691')\n",
    "# embedding = HuggingFaceEmbeddings(model_name=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['GraphRAG'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x129d33860>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.9, 'k': 10, 'params': {'topChunks': 3, 'topCommunities': 3, 'topOutsideRels': 10, 'topInsideRels': 10}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "lc_retrieval_query = \"\"\"\n",
    "WITH collect(node) as nodes\n",
    "// Entity - Text Unit Mapping\n",
    "WITH\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)<-[:HAS_ENTITY]->(c:__Chunk__)\n",
    "    WITH c, count(distinct n) as freq\n",
    "    RETURN c.content AS chunkText\n",
    "    ORDER BY freq DESC\n",
    "    LIMIT $topChunks\n",
    "} AS text_mapping,\n",
    "// Entity - Report Mapping\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[:IN_COMMUNITY]->(c:__Community__)\n",
    "    WHERE c.summary is not null\n",
    "    WITH c, c.rank as rank, c.weight AS weight\n",
    "    RETURN c.summary \n",
    "    ORDER BY rank, weight DESC\n",
    "    LIMIT $topCommunities\n",
    "} AS report_mapping,\n",
    "// Outside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r]-(m) \n",
    "    WHERE NOT m IN nodes and r.description is not null\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topOutsideRels\n",
    "} as outsideRels,\n",
    "// Inside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r]-(m) \n",
    "    WHERE m IN nodes and r.description is not null\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topInsideRels\n",
    "} as insideRels,\n",
    "// Entities description\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    match (n)\n",
    "    WHERE n.description is not null\n",
    "    RETURN n.description AS descriptionText\n",
    "} as entities\n",
    "// We don't have covariates or claims here\n",
    "RETURN {Chunks: text_mapping, Reports: report_mapping, \n",
    "       Relationships: outsideRels + insideRels, \n",
    "       Entities: entities} AS text, 1.0 AS score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "vectorstore = Neo4jVector.from_existing_graph(embedding=embedding, \n",
    "                                    index_name=\"embedding\",\n",
    "                                    node_label='__Entity__', \n",
    "                                    embedding_node_property='embedding', \n",
    "                                    text_node_properties=['id', 'description'],\n",
    "                                    retrieval_query=lc_retrieval_query)\n",
    "topChunks = 3\n",
    "topCommunities = 3\n",
    "topOutsideRels = 10\n",
    "topInsideRels = 10\n",
    "topEntities = 10\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"2wsx3edc\"\n",
    "\n",
    "local_search_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.9,\n",
    "                   'k': topEntities,\n",
    "                   'params': {\n",
    "                        \"topChunks\": topChunks,\n",
    "                        \"topCommunities\": topCommunities,\n",
    "                        \"topOutsideRels\": topOutsideRels,\n",
    "                        \"topInsideRels\": topInsideRels,\n",
    "                    }},\n",
    "    tags=['GraphRAG']\n",
    ")\n",
    "local_search_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(local_search_retriever.invoke('保險費暨保險單借款利息自動轉帳付款授權')[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.TWLF_Neo4jVector import TWLF_Neo4jVector\n",
    "vectorstore = TWLF_Neo4jVector.from_existing_graph(\n",
    "                                    embedding=embedding, \n",
    "                                    index_name=\"chunk_index\",\n",
    "                                    node_label='__Chunk__', \n",
    "                                    embedding_node_property='embedding', \n",
    "                                    text_node_properties=['content'])\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.9},\n",
    "    tags=['RAG']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(\n",
    "#     # model=\"llama3.1:70b-instruct-q8_0\",\n",
    "#     model='qwen2:72b-instruct-q8_0',\n",
    "# )\n",
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "你是一個有用的助手, 你的任務是整理提供的資訊, 使用長度與格式符合「multiple paragraphs」針對使用者的問題來回應,\n",
    "提供的資訊包含 檢索內容、圖譜資料庫相關節點與關係資訊, 無關的資訊直接忽略\n",
    "你必須使用繁體中文回應問題, 盡可能在500字內回應,\n",
    "如果提供的資訊沒有答案或是皆無關聯, 請直接說「找不到相關資訊」並結束, 不要捏造任何資訊,\n",
    "最終的回應將清理後的訊息合併成一個全面的答案，針對回應的長度和格式對所有關鍵點和含義進行解釋\n",
    "根據回應的長度和格式適當添加段落和評論。以Markdown格式撰寫回應。\n",
    "回應應保留原有的意思和使用的情態動詞，例如「應該」、「可以」或「將」。\n",
    "請確保使用繁體中文回答問題\n",
    "\n",
    "\n",
    "以下為檢索內容:\n",
    "\"{context}\"\n",
    "\n",
    "以下為圖譜資料庫相關節點(Entities)、關係(Relationships)、社群(Reports)、Chunks(內文節錄)資訊:\n",
    "\"{graph_result}\"\n",
    "\n",
    "問題: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vector_retriever, \"question\": RunnablePassthrough(), \"graph_result\": local_search_retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from tools.TokenCounter import num_tokens_from_string\n",
    "\n",
    "# q = '你對個人保險首續期繳費了解多少?'\n",
    "# print(num_tokens_from_string(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in rag_chain.stream(q):\n",
    "#     print(r, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \" # 給定一段聊天歷史和使用者的最新問題\n",
    "    \"which might reference context in the chat history, \" # 這個問題可能會引用聊天歷史中的上下文\n",
    "    \"formulate a standalone question which can be understood \" # 請將問題重新表述為一個獨立的問題，使其在沒有聊天歷史的情況下也能被理解\n",
    "    \"without the chat history. Do NOT answer the question, \" # 不要回答這個問題\n",
    "    \"just reformulate it if needed and otherwise return it as is.\" # 只需在必要時重新表述問題，否則原樣返回\n",
    "    \"請確保使用繁體中文回應\"\n",
    ")\n",
    "\n",
    "# 定義上下文解析的Chain\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = (\n",
    "    contextualize_q_prompt\n",
    "    | llm\n",
    "    | StrOutputParser().with_config({\n",
    "        'tags': ['contextualize_question']\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "store = {}\n",
    "\n",
    "    \n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 使用 RunnableParallel 來組織多個並行查詢\n",
    "context_and_search_chain = RunnableParallel(\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda inputs: vector_retriever.invoke(inputs)),\n",
    "        \"graph_result\": RunnableLambda(lambda inputs: local_search_retriever.invoke(inputs)),\n",
    "        \"question\": lambda x: x,  # 保留原始輸入\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    contextualize_chain\n",
    "    | context_and_search_chain\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser().with_config({\n",
    "        \"tags\": ['final_output']\n",
    "    })\n",
    ")\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for r in conversational_rag_chain.stream(\n",
    "#     {\"input\": \"常見的繳費方式為何有這三種?\"},\n",
    "#     config={\n",
    "#         \"configurable\": {\"session_id\": \"abc123\"}\n",
    "#     },  # constructs a key \"abc123\" in `store`.\n",
    "# ):\n",
    "#     print(r , end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChatHistory(BaseModel):\n",
    "    \"\"\"Chat history with the bot.\"\"\"\n",
    "    question: str\n",
    "    \n",
    "conversational_rag_chain = (\n",
    "  conversational_rag_chain | StrOutputParser()\n",
    ").with_types(input_type=ChatHistory)\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    conversational_rag_chain\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lagch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
