{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"2wsx3edc\"\n",
    "database = os.environ.get('NEO4J_DATABASE')\n",
    "graph = Neo4jGraph(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint='https://sales-chatbot-llm.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15',\n",
    "    azure_deployment='text-embedding-3-small',\n",
    "    openai_api_version='2023-05-15'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import Neo4jVector\n",
    "# # ! pip3 install -U langchain-huggingface\n",
    "# import os\n",
    "# os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/storage/models/embedding_models'\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# # Choose from https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# # embedding = HuggingFaceEmbeddings(model_name=\"lier007/xiaobu-embedding-v2\")\n",
    "\n",
    "# model_path = os.path.join(os.environ['SENTENCE_TRANSFORMERS_HOME'], 'models--lier007--xiaobu-embedding-v2/snapshots/ee0b4ecdf5eb449e8240f2e3de2e10eeae877691')\n",
    "# embedding = HuggingFaceEmbeddings(model_name=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['GraphRAG'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x15739efc0>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.9, 'k': 10, 'params': {'topChunks': 3, 'topCommunities': 3, 'topOutsideRels': 10, 'topInsideRels': 10}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "lc_retrieval_query = \"\"\"\n",
    "WITH collect(node) as nodes\n",
    "// Entity - Text Unit Mapping\n",
    "WITH\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)<-[:HAS_ENTITY]->(c:__Chunk__)<-[:HAS_CHILD]->(p:__Parent__)\n",
    "    WITH c, p, count(distinct n) as freq\n",
    "    RETURN p.content AS chunkText\n",
    "    ORDER BY freq DESC\n",
    "    LIMIT $topChunks\n",
    "} AS text_mapping,\n",
    "// Entity - Report Mapping\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[:IN_COMMUNITY]->(c:__Community__)\n",
    "    WHERE c.summary is not null\n",
    "    WITH c, c.rank as rank, c.weight AS weight\n",
    "    RETURN c.summary \n",
    "    ORDER BY rank, weight DESC\n",
    "    LIMIT $topCommunities\n",
    "} AS report_mapping,\n",
    "// Outside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r]-(m) \n",
    "    WHERE NOT m IN nodes and r.description is not null\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topOutsideRels\n",
    "} as outsideRels,\n",
    "// Inside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r]-(m) \n",
    "    WHERE m IN nodes and r.description is not null\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topInsideRels\n",
    "} as insideRels,\n",
    "// Entities description\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    match (n)\n",
    "    WHERE n.description is not null\n",
    "    RETURN n.description AS descriptionText\n",
    "} as entities\n",
    "// We don't have covariates or claims here\n",
    "RETURN {Chunks: text_mapping, Reports: report_mapping, \n",
    "       Relationships: outsideRels + insideRels, \n",
    "       Entities: entities} AS text, 1.0 AS score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "vectorstore = Neo4jVector.from_existing_graph(embedding=embedding, \n",
    "                                    index_name=\"embedding\",\n",
    "                                    node_label='__Entity__', \n",
    "                                    embedding_node_property='embedding', \n",
    "                                    text_node_properties=['id', 'description'],\n",
    "                                    retrieval_query=lc_retrieval_query)\n",
    "topChunks = 3\n",
    "topCommunities = 3\n",
    "topOutsideRels = 10\n",
    "topInsideRels = 10\n",
    "topEntities = 10\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"2wsx3edc\"\n",
    "\n",
    "local_search_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.9,\n",
    "                   'k': topEntities,\n",
    "                   'params': {\n",
    "                        \"topChunks\": topChunks,\n",
    "                        \"topCommunities\": topCommunities,\n",
    "                        \"topOutsideRels\": topOutsideRels,\n",
    "                        \"topInsideRels\": topInsideRels,\n",
    "                    }},\n",
    "    tags=['GraphRAG']\n",
    ")\n",
    "local_search_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(local_search_retriever.invoke('保險費暨保險單借款利息自動轉帳付款授權')[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.TWLF_Neo4jVector import TWLF_Neo4jVector\n",
    "vectorstore = TWLF_Neo4jVector.from_existing_graph(\n",
    "                                    embedding=embedding, \n",
    "                                    index_name=\"chunk_index\",\n",
    "                                    node_label='__Chunk__', \n",
    "                                    embedding_node_property='embedding', \n",
    "                                    text_node_properties=['content'])\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.9},\n",
    "    tags=['RAG']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1574703b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x157471ac0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x147f0bbf0>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x157470410>, model_kwargs={}, openai_api_key=SecretStr('**********'), azure_endpoint='https://sales-chatbot-llm.openai.azure.com/', deployment_name='GPT4o', openai_api_version='2023-03-15-preview', openai_api_type='azure')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(\n",
    "#     # model=\"llama3.1:70b-instruct-q8_0\",\n",
    "#     model='qwen2:72b-instruct-q8_0',\n",
    "# )\n",
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "你是一個有用的助手, 你的任務是整理提供的資訊, 使用長度與格式符合「multiple paragraphs」針對使用者的問題來回應,\n",
    "提供的資訊包含 檢索內容、圖譜資料庫相關節點與關係資訊, 無關的資訊直接忽略\n",
    "你必須使用繁體中文回應問題, 盡可能在500字內回應,\n",
    "若提供的資訊全部無關聯, 回應「找不到相關資訊」並結束, 不要捏造任何資訊,\n",
    "最終的回應將清理後的訊息合併成一個全面的答案，針對回應的長度和格式對所有關鍵點和含義進行解釋\n",
    "根據回應的長度和格式適當添加段落和評論。以Markdown格式撰寫回應。\n",
    "回應應保留原有的意思和使用的情態動詞，例如「應該」、「可以」或「將」。\n",
    "請確保使用繁體中文回答問題\n",
    "\n",
    "\n",
    "以下為檢索內容:\n",
    "\"{context}\"\n",
    "\n",
    "以下為圖譜資料庫相關節點(Entities)、關係(Relationships)、社群(Reports)、Chunks(內文節錄)資訊:\n",
    "\"{graph_result}\"\n",
    "\n",
    "問題: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vector_retriever, \"question\": RunnablePassthrough(), \"graph_result\": local_search_retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from tools.TokenCounter import num_tokens_from_string\n",
    "\n",
    "q = '首期匯款帳號有哪些銀行, 匯款帳號跟劃撥帳號是什麼?'\n",
    "print(num_tokens_from_string(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根據提供的資訊，首期保險費的匯款帳號可以分為台幣和外幣兩種，以下為詳細資訊：\n",
      "\n",
      "### 台幣匯款帳號\n",
      "- **匯款銀行**：中國信託商業銀行城中分行\n",
      "- **匯入帳號**：3562+1碼(類別)+10碼(保單號碼)+1碼(檢核碼)，共計16碼\n",
      "\n",
      "### 外幣匯款帳號\n",
      "- **匯款銀行**：CTBC Bank Co., Ltd (中國信託銀行)\n",
      "- **戶名**：台灣人壽保險股份有限公司\n",
      "- **SWIFT CODE**：CTCBTWTPXXX\n",
      "- **虛擬帳號**：\n",
      "  - **類型一**：95432+保單號碼10碼+檢查碼，共16碼\n",
      "  - **類型二**：95265+保單號碼10碼，共15碼\n",
      "\n",
      "這些帳號分別用於不同幣別的保單繳費，保戶需要根據保單類型選擇適合的匯款帳號進行繳費。請注意，匯款時需要在劃撥單或匯款單上註明保單號碼、要保人姓名、聯絡電話及相關款項清償金額。\n",
      "\n",
      "此外，保戶須自行負擔匯款手續費，並確保繳款人是保單關係人，如非保單關係人，需提供相關身份或關係證明文件。\n",
      "\n",
      "### 郵局劃撥\n",
      "- **劃撥帳號**：50120507 (至郵局劃撥至本公司之郵政劃撥帳號)\n",
      "\n",
      "### 匯款銀行詳細列表\n",
      "- **中國信託銀行**\n",
      "- **兆豐銀行**\n",
      "- **第一銀行**\n",
      "- **彰化銀行**\n",
      "- **台新銀行**\n",
      "- **元大銀行**\n",
      "- **永豐銀行**\n",
      "- **匯豐銀行（美金）**\n",
      "- **華南銀行**\n",
      "- **國泰世華**\n",
      "- **合作金庫**\n",
      "\n",
      "每個銀行都有各自的識別碼、SWIFT Code及詳細的跨行匯款規定，保戶可以依據自身需求選擇合適的銀行進行匯款。\n",
      "\n",
      "### 注意事項\n",
      "1. 請在劃撥單或匯款單上註明保單號碼、要保人姓名、聯絡電話及借款/墊繳本息清償金額。\n",
      "2. 繳費完成後，還款收據將由本公司郵寄給保戶。\n",
      "3. 若有需要，保戶可聯繫客服專線或訪問本公司會員網站查詢虛擬帳號轉換碼。\n",
      "\n",
      "希望這些資訊能夠幫助您順利完成首期保險費的匯款。"
     ]
    }
   ],
   "source": [
    "for r in rag_chain.stream(q):\n",
    "    print(r, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \" # 給定一段聊天歷史和使用者的最新問題\n",
    "    \"which might reference context in the chat history, \" # 這個問題可能會引用聊天歷史中的上下文\n",
    "    \"formulate a standalone question which can be understood \" # 請將問題重新表述為一個獨立的問題，使其在沒有聊天歷史的情況下也能被理解\n",
    "    \"without the chat history. Do NOT answer the question, \" # 不要回答這個問題\n",
    "    \"just reformulate it if needed and otherwise return it as is.\" # 只需在必要時重新表述問題，否則原樣返回\n",
    "    \"請確保使用繁體中文回應\"\n",
    ")\n",
    "\n",
    "# 定義上下文解析的Chain\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = (\n",
    "    contextualize_q_prompt\n",
    "    | llm\n",
    "    | StrOutputParser().with_config({\n",
    "        'tags': ['contextualize_question']\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "store = {}\n",
    "\n",
    "    \n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 使用 RunnableParallel 來組織多個並行查詢\n",
    "context_and_search_chain = RunnableParallel(\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda inputs: vector_retriever.invoke(inputs)),\n",
    "        \"graph_result\": RunnableLambda(lambda inputs: local_search_retriever.invoke(inputs)),\n",
    "        \"question\": lambda x: x,  # 保留原始輸入\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    contextualize_chain\n",
    "    | context_and_search_chain\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser().with_config({\n",
    "        \"tags\": ['final_output']\n",
    "    })\n",
    ")\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for r in conversational_rag_chain.stream(\n",
    "#     {\"input\": \"常見的繳費方式為何有這三種?\"},\n",
    "#     config={\n",
    "#         \"configurable\": {\"session_id\": \"abc123\"}\n",
    "#     },  # constructs a key \"abc123\" in `store`.\n",
    "# ):\n",
    "#     print(r , end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChatHistory(BaseModel):\n",
    "    \"\"\"Chat history with the bot.\"\"\"\n",
    "    question: str\n",
    "    \n",
    "conversational_rag_chain = (\n",
    "  conversational_rag_chain | StrOutputParser()\n",
    ").with_types(input_type=ChatHistory)\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    conversational_rag_chain\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lagch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
