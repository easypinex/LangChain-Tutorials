{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Build a Chatbot](https://python.langchain.com/v0.2/docs/tutorials/chatbot/)\n",
    "[Overview](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#overview \"Direct link to Overview\")\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "我們將探討如何設計和實現一個由 LLM 驅動的聊天機器人的示例。這個聊天機器人將能夠進行對話並記住之前的互動。\n",
    "\n",
    "請注意，我們構建的這個聊天機器人只會使用語言模型進行對話。不過，你可能還會對其他相關概念感興趣：\n",
    "\n",
    "-   [Conversational RAG](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/): 讓聊天機器人可以基於外部數據源進行對話\n",
    "-   [Agents](https://python.langchain.com/v0.2/docs/tutorials/agents/): 構建能夠執行操作的聊天機器人\n",
    "\n",
    "本教程將涵蓋基本內容，這些內容對後面更高級的主題也會有幫助，但如果你有興趣，也可以直接跳到那裡學習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import os\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "# model = AzureChatOpenAI(\n",
    "#     azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "#     azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "#     openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "# )\n",
    "# model\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model_name = \"llama3.1\"\n",
    "# model_name = \"jcai/taide-lx-7b-chat\"\n",
    "model = ChatOllama(model=model_name)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, 品至! How can I assist you today?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = model.invoke([HumanMessage(content=\"Hi, 我是品至.\")])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型本身並沒有任何狀態的概念。例如，如果你問一個後續問題："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'對不起，我們第一次見面就沒有交流過你的名字。'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Trace Sample:\n",
    "https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r\n",
    "'''\n",
    "response = model.invoke([HumanMessage(content=\"請問我的名字是什麼？\")])\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們來看一下這個示例 [LangSmith trace](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)。\n",
    "\n",
    "我們可以看到，模型並沒有將之前的對話輪次考慮在內，因此無法回答問題。這樣的聊天機器人體驗非常糟糕！\n",
    "\n",
    "為了解決這個問題，我們需要將整個對話歷史傳遞給模型。讓我們看看這樣做會發生什麼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你的名字是品至呦！'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi ,我叫品至\"),\n",
    "        AIMessage(content=\"你好，品至！有什麼我可以幫助您的嗎？\"),\n",
    "        HumanMessage(content=\"我的名字是什麼？\"),\n",
    "    ]\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們可以看到，我們得到了更好的回應！\n",
    "\n",
    "這就是支持聊天機器人進行對話交互的基本原理。那麼，我們該如何最佳化地實現這一點呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Message History](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#message-history \"Direct link to Message History\")\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "我們可以使用 Message History 類來封裝模型，使其具有狀態。這個類將跟蹤模型的輸入和輸出，並將它們存儲在某個數據庫中。未來的交互將加載這些消息，並將它們作為輸入的一部分傳遞給鏈。讓我們看看如何使用這個方法！\n",
    "\n",
    "首先，請確保安裝 `langchain-community`，因為我們將使用其中的一個集成來存儲消息歷史記錄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來，我們可以導入相關的類並設置我們的鏈，這個鏈將封裝模型並添加消息歷史記錄。關鍵的一部分是我們傳入的 get_session_history 函數。這個函數應該接受一個 session_id，並返回一個 Message History 對象。session_id 用於區分不同的對話，並應該在調用新的鏈時作為配置的一部分傳入（我們將展示如何操作）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！很高興遇到你，品至！想聊什麼嗎？'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"哈囉!我叫品至.\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'妳是品至!妳剛剛自己說的哦!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"我叫什麼?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太棒了！我們的聊天機器人現在能記住與我們相關的信息。如果我們更改配置中的 session_id，可以看到對話會從頭開始。\n",
    "這樣，我們就能支持聊天機器人同時與多個用戶進行對話了！\n",
    "\n",
    "目前，我們只是為模型添加了一個簡單的持久化層。接下來，我們可以通過添加提示模板，使聊天機器人變得更加複雜和個性化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prompt templates](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#prompt-templates \"Direct link to Prompt templates\")\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "提示模板有助於將原始的使用者資訊轉換成語言模型 (LLM) 能夠處理的格式。在這個案例中，原始的使用者輸入只是一則訊息，我們將其傳遞給 LLM。現在，讓我們使這個過程稍微複雜一些。首先，加入一則帶有自訂指令的系統訊息（但仍然以訊息作為輸入）。接下來，我們將添加更多除了訊息以外的輸入內容。\n",
    "\n",
    "首先，讓我們加入一則系統訊息。為此，我們將創建一個 ChatPromptTemplate。我們將利用 `MessagesPlaceholder` 來傳遞所有訊息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一個有用的助手. 盡你所能回答所有的問題.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！很高興認識你，品至！我是你的助手，隨時準備幫助你解答任何疑問或進行任何討論。您有什么需要我的幫助的嗎？'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! 我叫品至.\")]})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'欢迎珍妮！很高兴认识你！你有什么问题或话想说吗？'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! 我叫珍妮.\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你的名字是珍妮（Jenny）！'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"我的名字是什麼？\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一個有用的助手. 盡你所能回答所有的問題. 確保使用語言: {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請注意，這稍微改變了輸入類型------我們現在不再傳遞訊息列表，而是傳遞一個包含 `messages` 鍵的字典，其中該鍵包含了一個訊息列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！我是你的助手，很高興遇見你，艾麗絲（Alice）！怎麼了？有什麼事需要幫忙的嗎？'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! 我是愛麗絲.\")], \"language\": \"繁體中文\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們現在可以像之前一樣，將這個包裹在相同的訊息歷史對象中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, # prompt | model\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! 我是高進.\")], \"language\": \"繁體中文\"},\n",
    "    config=config,\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你刚才自己告訴我的，你叫做\"高進\"！'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"我叫什麼?\")], \"language\": \"繁體中文\"},\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Managing Conversation History](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#managing-conversation-history \"Direct link to Managing Conversation History\")\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "在構建聊天機器人時，一個重要的概念是如何管理對話歷史記錄。如果不加以管理，訊息列表會無限增長，最終可能會超出LLM的上下文窗口。因此，添加一個限制所傳遞訊息大小的步驟是很重要的。\n",
    "\n",
    "特別要注意的是，這個步驟應該在提示模板**之前**，但在從訊息歷史記錄中加載先前訊息**之後**執行。\n",
    "\n",
    "我們可以通過在提示之前添加一個簡單的步驟來適當修改 `messages` 鍵，然後將這個新鏈條包裹在訊息歷史記錄類中。\n",
    "\n",
    "LangChain 提供了一些內建的輔助工具來[管理訊息列表](https://python.langchain.com/v0.2/docs/how_to/#messages)。在這個案例中，我們將使用 [trim_messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/) 輔助工具來減少傳送給模型的訊息數量。這個工具允許我們指定要保留的代幣數量，以及其他參數，如是否始終保留系統訊息以及是否允許部分訊息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.1')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "model\n",
    "# ! pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是個有用的助手.'),\n",
       " HumanMessage(content='我喜歡香草冰淇淋!'),\n",
       " AIMessage(content='很好!'),\n",
       " HumanMessage(content='2 + 2 答案是什麼?'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='謝謝'),\n",
       " AIMessage(content='不客氣!'),\n",
       " HumanMessage(content='有趣嗎?'),\n",
       " AIMessage(content='有趣!')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=120,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是個有用的助手.\"),\n",
    "    HumanMessage(content=\"hi! 我叫品至.\"),\n",
    "    AIMessage(content=\"Hi!\"),\n",
    "    HumanMessage(content=\"我喜歡香草冰淇淋!\"),\n",
    "    AIMessage(content=\"很好!\"),\n",
    "    HumanMessage(content=\"2 + 2 答案是什麼?\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"謝謝\"),\n",
    "    AIMessage(content=\"不客氣!\"),\n",
    "    HumanMessage(content=\"有趣嗎?\"),\n",
    "    AIMessage(content=\"有趣!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要在我們的鏈條中使用它，我們只需要在將 `messages` 輸入傳遞給提示模板之前運行修剪器。\n",
    "\n",
    "現在，如果我們嘗試詢問模型我們的名字，它可能不會知道，因為我們已經修剪了那部分的聊天歷史記錄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你沒有告訴我你的名字，我也不知道。這裡的對話才剛開始，你可以先自我介紹一下！'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"我的名字是什麼?\")],\n",
    "        \"language\": \"繁體中文\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，如果我們詢問最近幾條訊息中的資訊，它還是會記得的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你剛才問的是 \"2 + 2 答案是什麼?\" 的問題。然後你也跟我說了 \"謝謝\"，我回覆了 \"不客氣!\"，然後你又問了 \"有趣嗎?\"！'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"剛剛我問了什麼？\")],\n",
    "        \"language\": \"繁體中文\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，讓我們將這個包裹在訊息歷史記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, # trimer | prompt | model\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我不知道你的名字!你能告訴我你的名字嗎?'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"我的名字是什麼?\")],\n",
    "        \"language\": \"繁體中文\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如預期的那樣，我們最初提到名字的訊息已經被修剪掉了。此外，現在聊天歷史記錄中有兩條新訊息（我們的最新問題和最新回應）。這意味著我們對話歷史中原本可訪問的更多資訊也不再可用！在這個例子中，我們最初的數學問題也已經從歷史記錄中被修剪掉了，所以模型已經不再知道它的存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Streaming](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#streaming \"Direct link to Streaming\")\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "現在我們已經有了一個功能完善的聊天機器人。然而，對於聊天機器人應用程序來說，一個*非常*重要的用戶體驗考量是串流（Streaming）。由於LLMs有時可能需要一段時間才能回應，因此為了提升用戶體驗，大多數應用程序會在生成每個代幣時立即串流回傳，這樣用戶就能看到進展。\n",
    "\n",
    "其實這個功能非常容易實現！\n",
    "\n",
    "所有的鏈條都提供了一個 `.stream` 方法，使用訊息歷史記錄的鏈條也不例外。我們只需使用該方法即可獲得串流回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"HI. 我是高進. 請跟我說我一個短笑話. 最後跟我說好笑在哪裡\")],\n",
    "        \"language\": \"繁體中文\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Next Steps](https://python.langchain.com/v0.2/docs/tutorials/chatbot/#next-steps \"Direct link to Next Steps\")\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "現在您已經了解如何在 LangChain 中創建聊天機器人的基礎知識，您可能會對一些更高級的教程感興趣：\n",
    "\n",
    "-   [Conversational RAG](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/): 使聊天機器人能夠基於外部數據源提供對話體驗\n",
    "-   [Agents](https://python.langchain.com/v0.2/docs/tutorials/agents/): 構建能夠執行操作的聊天機器人\n",
    "\n",
    "如果您想深入了解一些具體內容，以下內容值得查看：\n",
    "\n",
    "-   [Streaming](https://python.langchain.com/v0.2/docs/how_to/streaming/): 串流對於聊天應用至關重要\n",
    "-   [How to add message history](https://python.langchain.com/v0.2/docs/how_to/message_history/): 更深入地了解與消息歷史相關的一切\n",
    "-   [How to manage large message history](https://python.langchain.com/v0.2/docs/how_to/trim_messages/): 更多管理大型聊天歷史記錄的技巧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lagch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
